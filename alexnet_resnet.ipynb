{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle as pk\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from structures import LateralInhibition, LIBlock\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m IMAGENET_MEAN \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m IMAGENET_STD \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m], dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      4\u001b[0m TRAIN_NORMALIZE \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      5\u001b[0m     [\n\u001b[1;32m      6\u001b[0m         transforms\u001b[39m.\u001b[39mResize((\u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "TRAIN_NORMALIZE = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(0.75, 1.33)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN.tolist(), std=IMAGENET_STD.tolist()),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "TEST_NORMALIZE = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN.tolist(), std=IMAGENET_STD.tolist()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def deprocess(img):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(mean=[0, 0, 0], std=(1.0 / IMAGENET_STD).tolist()),\n",
    "            transforms.Normalize(mean=(-IMAGENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "            transforms.ToPILImage(),\n",
    "        ]\n",
    "    )\n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def load_datas(batch_size=128, root=\"\"):\n",
    "    train_dataset = ImageFolder(root=f\"{root}imagenet-mini/train\", transform=TRAIN_NORMALIZE)\n",
    "    test_dataset = ImageFolder(root=f\"{root}imagenet-mini/val\", transform=TEST_NORMALIZE)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_datas()\n",
    "TRAIN_SIZE, TEST_SIZE = len(train_loader.dataset), len(test_loader.dataset)\n",
    "\n",
    "print(f\"train dataset: {TRAIN_SIZE}, test_dataset: {TEST_SIZE}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, num_epochs=90, batch_accumulation=2, eval_freq=5, comment=\"\", verbose=0\n",
    "):\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "\n",
    "\n",
    "    # Setup training\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    # loss and accuracy records\n",
    "    train_loss_record = []\n",
    "    train_accuracy_record = []\n",
    "    test_loss_record = []\n",
    "    test_accuracy_record = []\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            # compute batch accuracy\n",
    "            correct += torch.sum(labels == pred).sum().item()\n",
    "\n",
    "            # compute the loss and accumulate it\n",
    "            loss = criterion(output, labels) / batch_accumulation\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % batch_accumulation:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # decrease learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss /= TRAIN_SIZE\n",
    "        accuracy = correct / TRAIN_SIZE\n",
    "\n",
    "        writer.add_scalar(\"Train/loss\", total_loss, epoch)\n",
    "        writer.add_scalar(\"Train/accuracy\", accuracy, epoch)\n",
    "\n",
    "        train_loss_record.append(total_loss.item())\n",
    "        train_accuracy_record.append(accuracy)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[{epoch + 1:2d}/{num_epochs}] loss_train: {total_loss:.2E} accuracy_train: {accuracy:.2%}\"\n",
    "            )\n",
    "\n",
    "        if (epoch + 1) % eval_freq:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_loss, correct = 0, 0\n",
    "\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = model(images)\n",
    "                pred = output.argmax(dim=1)\n",
    "\n",
    "                correct += torch.sum(labels == pred).sum().item()\n",
    "\n",
    "                loss = criterion(output, labels)\n",
    "                test_loss += loss\n",
    "\n",
    "            test_loss /= TEST_SIZE\n",
    "            accuracy = correct / TEST_SIZE\n",
    "\n",
    "            writer.add_scalar(\"Test/loss\", test_loss, epoch)\n",
    "            writer.add_scalar(\"Test/accuracy\", accuracy, epoch)\n",
    "\n",
    "            test_loss_record.append(test_loss.item())\n",
    "            test_accuracy_record.append(accuracy)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"loss_test: {total_loss:.2E} accuracy_test: {accuracy:.2%}\")\n",
    "\n",
    "    # saving logs and model\n",
    "    log_path = \"logs/\"\n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    # find last model run id\n",
    "    max_run_id = 0\n",
    "    for path in glob.glob(os.path.join(log_path, model_name + \"_[0-9]*\")):\n",
    "        file_name = os.path.basename(path)\n",
    "        ext = file_name.split(\"_\")[-1]\n",
    "        if (\n",
    "            model_name == \"_\".join(file_name.split(\"_\")[:-1])\n",
    "            and ext.isdigit()\n",
    "            and int(ext) > max_run_id\n",
    "        ):\n",
    "            max_run_id = int(ext)\n",
    "\n",
    "    file_name = model_name + f\"_{max_run_id+1}\"\n",
    "    save_dir = os.path.join(log_path, file_name)\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "    log = {\n",
    "        \"train_loss\": train_loss_record,\n",
    "        \"train_accuracy\": train_accuracy_record,\n",
    "        \"test_loss\": test_loss_record,\n",
    "        \"test_accuracy\": test_accuracy_record,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(save_dir, \"log.pkl\"), \"wb\") as f:\n",
    "        pk.dump(log, f)\n",
    "\n",
    "    torch.save(model, os.path.join(save_dir, \"model.pth\"))\n",
    "\n",
    "    print(f\"Model saved in: {save_dir}\")\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model = model.eval()\n",
    "    correct = 0\n",
    "    for _, (images, labels) in enumerate(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        pred = torch.argmax(model(images), dim=1)\n",
    "        correct += torch.sum(labels == pred).sum().item()\n",
    "\n",
    "    print(f\"accuracy: {correct / len(data_loader.dataset):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_heatmap(model):\n",
    "    assert hasattr(model, \"log\")\n",
    "\n",
    "    weights = [t.flatten().detach().cpu() for t in model.log[\"W\"]]\n",
    "\n",
    "    w_min = torch.cat(weights).min()\n",
    "    w_max = torch.cat(weights).max()\n",
    "\n",
    "    bins = np.linspace(w_min, w_max, 20)\n",
    "    counts = np.zeros((len(weights), len(bins) - 1))\n",
    "\n",
    "    for i, W in enumerate(weights):\n",
    "        for j in range(len(bins) - 1):\n",
    "            counts[i][j] = ((W >= bins[j]) & (W < bins[j + 1])).sum().item()\n",
    "\n",
    "    counts /= counts.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    labels = [r\"$LI_{}$\".format(i + 1) for i in range(counts.shape[0])]\n",
    "    xticks = [f\"{bins[i] + bins[i+1] / 2:.2f}\" for i in range(bins.size - 1)]\n",
    "\n",
    "    ax.imshow(counts, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "    ax.set_xticks(np.arange(counts.shape[1]), xticks, rotation=45)\n",
    "    ax.set_yticks(np.arange(counts.shape[0]), labels)\n",
    "\n",
    "    plt.title(\"LI-layers weights heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_log(model_path, key=\"accuracy\"):\n",
    "    with open(f\"{model_path}/log.pkl\", \"rb\") as f:\n",
    "        log = pk.load(f)\n",
    "\n",
    "    model_name = model_path.split(\"/\")[1].split(\"_\")[0]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax[0].plot(log[f\"train_{key}\"], label=f\"{model_name}\")\n",
    "    ax[0].set_xlabel(\"epoch\")\n",
    "    ax[0].set_ylabel(key)\n",
    "    ax[0].set_title(\"fTrain {key}\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(log[f\"test_{key}\"], label=f\"{model_name}\")\n",
    "    ax[1].set_xlabel(\"epoch\")\n",
    "    ax[1].set_ylabel(key)\n",
    "    ax[1].set_title(f\"Test {key}\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_datas(batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 03:59:27.671784: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-13 03:59:28.338955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-13 03:59:28.339076: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-13 03:59:28.339095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 272/272 [00:34<00:00,  7.87it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  8.98it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  8.98it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.07it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  8.95it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.05it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.06it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.05it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.03it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.05it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.05it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.02it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.03it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.10it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.04it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.03it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.10it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.07it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.07it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.07it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.03it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.06it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.10it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.10it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.07it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.06it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.07it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.03it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.10it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.14it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.06it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.10it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.14it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.08it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  8.98it/s]\n",
      "100%|██████████| 272/272 [00:34<00:00,  7.98it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.14it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.04it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.14it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.03it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.14it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:30<00:00,  9.05it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.09it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.13it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.12it/s]\n",
      "100%|██████████| 272/272 [00:29<00:00,  9.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in: logs/AlexNet_2\n"
     ]
    }
   ],
   "source": [
    "alexnet = models.alexnet(weights=None).to(device)\n",
    "train_model(alexnet, 90, 2, 5, f\"_AlexNet\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"logs/AlexNet_1\"\n",
    "\n",
    "model = torch.load(f\"{model_path}/model.pth\")\n",
    "evaluate(model, test_loader)\n",
    "plot_log(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet+LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetLI(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(AlexNetLI, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(weights=weights)\n",
    "\n",
    "        self.log = {\"W\": [], \"m\": [], \"v\": [], \"b\": []}\n",
    "\n",
    "        # Rebuild alexnet features, by adding a LI layer after each convolutions'\n",
    "        # activation function\n",
    "        features = list(alexnet.features.children())\n",
    "        new_features = []\n",
    "\n",
    "        for i, l in enumerate(features):\n",
    "            new_features.append(l)\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                li = LateralInhibition(features[i - 1].out_channels)\n",
    "                new_features.append(li)\n",
    "                self.log[\"W\"].append(li.weights)\n",
    "                self.log[\"m\"].append(li.m)\n",
    "                self.log[\"v\"].append(li.v)\n",
    "                self.log[\"b\"].append(li.b)\n",
    "\n",
    "        self.features = nn.Sequential(*new_features)\n",
    "\n",
    "        # Copy all the non-convolutional parts of AlexNet\n",
    "        self.avg_pool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 10:59:38.046178: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-13 10:59:38.167075: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-13 10:59:38.696955: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jules/.mujoco/mjpro150/bin\n",
      "2023-02-13 10:59:38.697056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/jules/.mujoco/mjpro150/bin\n",
      "2023-02-13 10:59:38.697063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "100%|██████████| 272/272 [00:50<00:00,  5.34it/s]\n",
      "100%|██████████| 272/272 [00:37<00:00,  7.28it/s]\n",
      "100%|██████████| 272/272 [00:35<00:00,  7.56it/s]\n",
      "100%|██████████| 272/272 [00:35<00:00,  7.57it/s]\n",
      "100%|██████████| 272/272 [00:36<00:00,  7.53it/s]\n",
      " 96%|█████████▌| 260/272 [00:36<00:01,  7.08it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 8257) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m alexnetLI \u001b[39m=\u001b[39m AlexNetLI(weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 2\u001b[0m train_model(alexnetLI, \u001b[39m50\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m_AlexNet+LI\u001b[39;49m\u001b[39m\"\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, batch_accumulation, eval_freq, comment, verbose)\u001b[0m\n\u001b[1;32m     35\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels) \u001b[39m/\u001b[39m batch_accumulation\n\u001b[1;32m     36\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m---> 37\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     39\u001b[0m \u001b[39mif\u001b[39;00m (batch_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m batch_accumulation:\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deepdac/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py:66\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     64\u001b[0m     \u001b[39m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     _error_if_any_worker_fails()\n\u001b[1;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m previous_handler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m         \u001b[39massert\u001b[39;00m callable(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 8257) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "alexnetLI = AlexNetLI(weights=None).to(device)\n",
    "train_model(alexnetLI, 50, 2, 5, f\"_AlexNet+LI\", verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"logs/AlexNetLI_1\"\n",
    "\n",
    "model = torch.load(f\"{model_path}/model.pth\")\n",
    "evaluate(model, test_loader)\n",
    "plot_log(model_path)\n",
    "plot_weights_heatmap(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet+BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetBatchNorm(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(AlexNetBatchNorm, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(weights=weights)\n",
    "\n",
    "        # Rebuild alexnet features, by adding a BatchNorm layer after each convolutions'\n",
    "        # activation function\n",
    "        features = list(alexnet.features.children())\n",
    "        new_features = []\n",
    "\n",
    "        for i, l in enumerate(features):\n",
    "            new_features.append(l)\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                new_features.append(nn.BatchNorm2d(features[i-1].out_channels))\n",
    "\n",
    "        self.features = nn.Sequential(*new_features)\n",
    "\n",
    "        # Copy all the non-convolutional parts of AlexNet\n",
    "        self.avg_pool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    alexnetBN = AlexNetBatchNorm().to(device)\n",
    "    train_model(alexnetBN, comment=\"_AlexNet+BatchNorm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet+GroupNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetGroupNorm(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(AlexNetGroupNorm, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(weights=weights)\n",
    "\n",
    "        # Rebuild alexnet features, by adding a LayerNorm layer after each convolutions'\n",
    "        # activation function\n",
    "        features = list(alexnet.features.children())\n",
    "        new_features = []\n",
    "        \n",
    "        for i, l in enumerate(features):\n",
    "            new_features.append(l)\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                new_features.append(nn.GroupNorm(1, features[i-1].out_channels))\n",
    "\n",
    "        self.features = nn.Sequential(*new_features)\n",
    "\n",
    "        # Copy all the non-convolutional parts of AlexNet\n",
    "        self.avg_pool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    alexnetGN = AlexNetGroupNorm().to(device)\n",
    "    train_model(alexnetGN, comment=\"_AlexNet+GroupNorm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_datas(batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    resnet18 = models.resnet18(weights=None).to(device)\n",
    "    train_model(resnet18, num_epochs=100, comment=\"_ResNet18\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet+LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLI(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(ResNetLI, self).__init__() \n",
    "        \n",
    "        resnet = models.resnet18(weights=weights)\n",
    "\n",
    "        self.log = {\"W\": [], \"m\": [], \"v\": [], \"b\": []}\n",
    "        \n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.li1 = LateralInhibition(self.conv1.out_channels)\n",
    "        self.maxpool = resnet.maxpool\n",
    "\n",
    "        self.layer1 = self.convert_layer_blocks(resnet.layer1)\n",
    "        self.layer2 = self.convert_layer_blocks(resnet.layer2)\n",
    "        self.layer3 = self.convert_layer_blocks(resnet.layer3)\n",
    "        self.layer4 = self.convert_layer_blocks(resnet.layer4)\n",
    "        \n",
    "        # Copy all the non-convolutional parts of ResNet\n",
    "        self.avgpool = resnet.avgpool\n",
    "        self.fc = resnet.fc\n",
    "\n",
    "        # Add the \"plain\" li params to the log\n",
    "        self.log[\"W\"].append(self.li1.weights)\n",
    "        self.log[\"m\"].append(self.li1.m)\n",
    "        self.log[\"v\"].append(self.li1.v)\n",
    "        self.log[\"b\"].append(self.li1.b)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.li1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def convert_layer_blocks(self, layer: nn.Sequential):\n",
    "        new_layer = []\n",
    "\n",
    "        for l in layer:\n",
    "            if isinstance(l, BasicBlock):\n",
    "                liblock = LIBlock(l)\n",
    "                new_layer.append(liblock)\n",
    "                \n",
    "                self.log[\"W\"].append(liblock.li.weights)\n",
    "                self.log[\"m\"].append(liblock.li.m)\n",
    "                self.log[\"v\"].append(liblock.li.v)\n",
    "                self.log[\"b\"].append(liblock.li.b)\n",
    "            else:\n",
    "                new_layer.append(l)\n",
    "\n",
    "        return nn.Sequential(*new_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    resnetLI = ResNetLI().to(device)\n",
    "    train_model(resnetLI, num_epochs=100, comment=\"_ResNet18+LI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = enumerate(train_loader)\n",
    "idx, (image, label) = next(batch)\n",
    "\n",
    "image, label = image.to(device), label.to(device)\n",
    "\n",
    "LI = LateralInhibition().cuda()\n",
    "output = LI(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 13\n",
    "img = deprocess(image[idx])\n",
    "display(img)\n",
    "\n",
    "li_img = deprocess(output[idx])\n",
    "display(li_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2e2b763ce854165d4087dd5d7e36e1631e69a252c45c2f7bdfd54f03b8f37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

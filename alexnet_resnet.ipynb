{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from structures import LateralInhibition, LIBlock\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"using {device}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and normalize ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset: 34745, test_dataset: 3923\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "TRAIN_NORMALIZE = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN.tolist(), std=IMAGENET_STD.tolist()),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0), ratio=(0.75, 1.33)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "TEST_NORMALIZE = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN.tolist(), std=IMAGENET_STD.tolist()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def deprocess(img):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(mean=[0, 0, 0], std=(1.0 / IMAGENET_STD).tolist()),\n",
    "            transforms.Normalize(mean=(-IMAGENET_MEAN).tolist(), std=[1, 1, 1]),\n",
    "            transforms.ToPILImage(),\n",
    "        ]\n",
    "    )\n",
    "    return transform(img)\n",
    "\n",
    "\n",
    "def load_datas(batch_size=128):\n",
    "    train_dataset = ImageFolder(root=\"imagenet-mini/train\", transform=TRAIN_NORMALIZE)\n",
    "    test_dataset = ImageFolder(root=\"imagenet-mini/val\", transform=TEST_NORMALIZE)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = load_datas()\n",
    "TRAIN_SIZE, TEST_SIZE = len(train_loader.dataset), len(test_loader.dataset)\n",
    "\n",
    "print(f\"train dataset: {TRAIN_SIZE}, test_dataset: {TEST_SIZE}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, max_epochs=10, batch_accumulation=2, eval_freq=2, comment=\"\"):\n",
    "    writer = SummaryWriter(comment=comment)\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        total_loss, correct = 0, 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = model(images)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            loss = criterion(output, labels) / batch_accumulation\n",
    "            total_loss += loss\n",
    "\n",
    "            correct += torch.sum(labels == pred).sum().item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if (batch_idx + 1) % batch_accumulation:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss /= TRAIN_SIZE\n",
    "        accuracy = correct / TRAIN_SIZE\n",
    "\n",
    "        writer.add_scalar(\"Train/loss\", total_loss, epoch)\n",
    "        writer.add_scalar(\"Train/accuracy\", accuracy, epoch)\n",
    "\n",
    "        print(\n",
    "            f\"[{epoch + 1:2d}/{max_epochs}] loss_train: {total_loss:.2E} accuracy_train: {accuracy:.2%}\"\n",
    "        )\n",
    "\n",
    "        if not epoch % eval_freq:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_loss, correct = 0, 0\n",
    "\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = model(images)\n",
    "                pred = output.argmax(dim=1)\n",
    "\n",
    "                loss = criterion(output, labels)\n",
    "                test_loss += loss\n",
    "\n",
    "                correct += torch.sum(labels == pred).sum().item()\n",
    "\n",
    "            test_loss /= TEST_SIZE\n",
    "            accuracy = correct / TEST_SIZE\n",
    "\n",
    "            writer.add_scalar(\"Test/loss\", test_loss, epoch)\n",
    "            writer.add_scalar(\"Test/accuracy\", accuracy, epoch)\n",
    "\n",
    "        print(f\"loss_test: {total_loss:.2E} accuracy_test: {accuracy:.2%}\")\n",
    "\n",
    "        if hasattr(model, \"log\"):\n",
    "            W = torch.concatenate([t.flatten() for t in model.log[\"W\"]])\n",
    "            m = torch.tensor([t.item() for t in model.log[\"m\"]])\n",
    "            v = torch.tensor([t.item() for t in model.log[\"v\"]])\n",
    "            b = torch.tensor([t.item() for t in model.log[\"b\"]])\n",
    "\n",
    "            print(\"LI-layers params (avg, min, max):\")\n",
    "            print(f\"* W: ({W.mean():.2f}, {W.min():.2f}, {W.max():.2f})\")\n",
    "            print(f\"* m: ({m.mean():.2f}, {m.min():.2f}, {b.max():.2f})\")\n",
    "            print(f\"* v: ({v.mean():.2f}, {v.min():.2f}, {v.max():.2f})\")\n",
    "            print(f\"* b: ({b.mean():.2f}, {b.min():.2f}, {b.max():.2f})\")\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, mode=\"train\"):\n",
    "    correct = 0\n",
    "    for _, (images, labels) in enumerate(data_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        pred = torch.argmax(model(images), dim=1)\n",
    "        correct += torch.sum(labels == pred).sum().item()\n",
    "\n",
    "    print(f\"{mode} accuracy: {correct / len(data_loader.dataset):.2%}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_datas(batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet = models.alexnet(weights=\"DEFAULT\").to(device)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 53.05%\n"
     ]
    }
   ],
   "source": [
    "evaluate(alexnet, train_loader, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 52.23%\n"
     ]
    }
   ],
   "source": [
    "evaluate(alexnet, test_loader, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] loss_train: 1.15E-02 accuracy_train: 37.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2/10] loss_train: 1.01E-02 accuracy_train: 42.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 1.01E-02 accuracy_test: 37.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:31<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3/10] loss_train: 9.50E-03 accuracy_train: 45.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4/10] loss_train: 8.76E-03 accuracy_train: 49.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 8.76E-03 accuracy_test: 34.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5/10] loss_train: 8.18E-03 accuracy_train: 51.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/10] loss_train: 7.78E-03 accuracy_train: 53.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 7.78E-03 accuracy_test: 33.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7/10] loss_train: 7.37E-03 accuracy_train: 55.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8/10] loss_train: 7.14E-03 accuracy_train: 57.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 7.14E-03 accuracy_test: 32.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:33<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9/10] loss_train: 6.68E-03 accuracy_train: 59.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:32<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] loss_train: 6.57E-03 accuracy_train: 60.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 6.57E-03 accuracy_test: 31.68%\n"
     ]
    }
   ],
   "source": [
    "alexnet = models.alexnet(weights=\"DEFAULT\").to(device)\n",
    "train_model(alexnet, 10, 2, 2, f\"ALEXNET\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet+LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexnetLI(nn.Module):\n",
    "    def __init__(self, weights=\"DEFAULT\", li_weights=\"zeros\", freeze=False):\n",
    "        super(AlexnetLI, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(weights=weights)\n",
    "\n",
    "        if freeze:\n",
    "            for param in alexnet.parameters():\n",
    "                param.require_grad = False\n",
    "\n",
    "        self.log = {\"W\": [], \"m\": [], \"v\": [], \"b\": []}\n",
    "\n",
    "        # Rebuild alexnet features, by adding a LI layer after each convolutions'\n",
    "        # activation function\n",
    "        features = list(alexnet.features.children())\n",
    "        new_features = []\n",
    "\n",
    "        for i, l in enumerate(features):\n",
    "            new_features.append(l)\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                li = LateralInhibition(features[i - 1].out_channels, weights=li_weights)\n",
    "                new_features.append(li)\n",
    "                self.log[\"W\"].append(li.weights)\n",
    "                self.log[\"m\"].append(li.m)\n",
    "                self.log[\"v\"].append(li.v)\n",
    "                self.log[\"b\"].append(li.b)\n",
    "\n",
    "        self.features = nn.Sequential(*new_features)\n",
    "\n",
    "        # Copy all the non-convolutional parts of AlexNet\n",
    "        self.avg_pool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:52<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] loss_train: 1.12E-02 accuracy_train: 38.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:51<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2/10] loss_train: 1.01E-02 accuracy_train: 43.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 1.01E-02 accuracy_test: 36.45%\n",
      "LI layers params (avg, min, max) :\n",
      "* W: (0.00, -0.02, 0.10)\n",
      "* m: (-0.09, -0.16, -0.07)\n",
      "* v: (0.96, 0.73, 1.11)\n",
      "* b: (-0.10, -0.16, -0.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:52<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3/10] loss_train: 9.32E-03 accuracy_train: 46.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:52<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4/10] loss_train: 8.55E-03 accuracy_train: 50.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 8.55E-03 accuracy_test: 36.04%\n",
      "LI layers params (avg, min, max) :\n",
      "* W: (0.00, -0.03, 0.15)\n",
      "* m: (-0.11, -0.24, -0.09)\n",
      "* v: (0.97, 0.63, 1.18)\n",
      "* b: (-0.13, -0.23, -0.09)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:52<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5/10] loss_train: 8.08E-03 accuracy_train: 52.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:51<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/10] loss_train: 7.56E-03 accuracy_train: 54.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 7.56E-03 accuracy_test: 34.44%\n",
      "LI layers params (avg, min, max) :\n",
      "* W: (0.00, -0.04, 0.20)\n",
      "* m: (-0.15, -0.28, -0.13)\n",
      "* v: (0.98, 0.60, 1.25)\n",
      "* b: (-0.17, -0.27, -0.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:52<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7/10] loss_train: 7.16E-03 accuracy_train: 57.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:51<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8/10] loss_train: 6.70E-03 accuracy_train: 59.29%\n",
      "loss_test: 6.70E-03 accuracy_test: 31.76%\n",
      "LI layers params (avg, min, max) :\n",
      "* W: (0.00, -0.05, 0.22)\n",
      "* m: (-0.16, -0.30, -0.13)\n",
      "* v: (0.98, 0.57, 1.29)\n",
      "* b: (-0.19, -0.29, -0.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:51<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9/10] loss_train: 6.41E-03 accuracy_train: 61.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:52<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] loss_train: 6.11E-03 accuracy_train: 62.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 6.11E-03 accuracy_test: 30.89%\n",
      "LI layers params (avg, min, max) :\n",
      "* W: (0.00, -0.05, 0.25)\n",
      "* m: (-0.18, -0.35, -0.15)\n",
      "* v: (0.97, 0.52, 1.31)\n",
      "* b: (-0.21, -0.33, -0.15)\n"
     ]
    }
   ],
   "source": [
    "alexnetLI = AlexnetLI().to(device)\n",
    "train_model(alexnetLI, 10, 2, 2, f\"ALEXNET_LI 256 log\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet+BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexnetBatchNorm(nn.Module):\n",
    "    def __init__(self, weights=\"DEFAULT\"):\n",
    "        super(AlexnetBatchNorm, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(weights=weights)\n",
    "\n",
    "        # Rebuild alexnet features, by adding a BatchNorm layer after each convolutions'\n",
    "        # activation function\n",
    "        features = list(alexnet.features.children())\n",
    "        new_features = []\n",
    "\n",
    "        for i, l in enumerate(features):\n",
    "            new_features.append(l)\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                new_features.append(nn.BatchNorm2d(features[i-1].out_channels))\n",
    "\n",
    "        self.features = nn.Sequential(*new_features)\n",
    "\n",
    "        # Copy all the non-convolutional parts of AlexNet\n",
    "        self.avg_pool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:39<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] loss_train: 1.28E-02 accuracy_train: 32.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:36<00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2/10] loss_train: 1.03E-02 accuracy_train: 42.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 1.03E-02 accuracy_test: 35.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:37<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3/10] loss_train: 9.16E-03 accuracy_train: 47.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:34<00:00,  8.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4/10] loss_train: 8.37E-03 accuracy_train: 51.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 8.37E-03 accuracy_test: 34.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:35<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5/10] loss_train: 7.65E-03 accuracy_train: 54.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:34<00:00,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/10] loss_train: 7.15E-03 accuracy_train: 57.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 7.15E-03 accuracy_test: 34.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:35<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7/10] loss_train: 6.74E-03 accuracy_train: 59.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:34<00:00,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8/10] loss_train: 6.39E-03 accuracy_train: 61.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 6.39E-03 accuracy_test: 33.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:35<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9/10] loss_train: 5.97E-03 accuracy_train: 64.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:35<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] loss_train: 5.77E-03 accuracy_train: 65.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 5.77E-03 accuracy_test: 32.09%\n"
     ]
    }
   ],
   "source": [
    "alexnetBN = AlexnetBatchNorm().to(device)\n",
    "train_model(alexnetBN, 10, 2, 2, \"ALEXNET_BatchNorm 256\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alexnet+GroupNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexnetGroupNorm(nn.Module):\n",
    "    def __init__(self, weights=\"DEFAULT\"):\n",
    "        super(AlexnetGroupNorm, self).__init__()\n",
    "\n",
    "        alexnet = models.alexnet(weights=weights)\n",
    "\n",
    "        # Rebuild alexnet features, by adding a LayerNorm layer after each convolutions'\n",
    "        # activation function\n",
    "        features = list(alexnet.features.children())\n",
    "        new_features = []\n",
    "        \n",
    "        for i, l in enumerate(features):\n",
    "            new_features.append(l)\n",
    "            if isinstance(l, nn.ReLU):\n",
    "                new_features.append(nn.GroupNorm(1, features[i-1].out_channels))\n",
    "\n",
    "        self.features = nn.Sequential(*new_features)\n",
    "\n",
    "        # Copy all the non-convolutional parts of AlexNet\n",
    "        self.avg_pool = alexnet.avgpool\n",
    "        self.classifier = alexnet.classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/10] loss_train: 1.29E-02 accuracy_train: 32.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2/10] loss_train: 1.04E-02 accuracy_train: 41.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 1.04E-02 accuracy_test: 37.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:41<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3/10] loss_train: 9.26E-03 accuracy_train: 46.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:43<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4/10] loss_train: 8.60E-03 accuracy_train: 50.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 8.60E-03 accuracy_test: 35.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5/10] loss_train: 7.95E-03 accuracy_train: 53.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6/10] loss_train: 7.30E-03 accuracy_train: 56.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 7.30E-03 accuracy_test: 35.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7/10] loss_train: 6.90E-03 accuracy_train: 58.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:39<00:00,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8/10] loss_train: 6.42E-03 accuracy_train: 61.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 6.42E-03 accuracy_test: 34.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9/10] loss_train: 6.17E-03 accuracy_train: 62.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 272/272 [00:40<00:00,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10/10] loss_train: 5.73E-03 accuracy_train: 65.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_test: 5.73E-03 accuracy_test: 32.04%\n"
     ]
    }
   ],
   "source": [
    "alexnetGN = AlexnetGroupNorm().to(device)\n",
    "train_model(alexnetGN, 10, 2, 2, \"ALEXNET_GroupNorm 256\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_datas(batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = models.resnet18(weights=\"DEFAULT\").to(device)\n",
    "resnet18.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 65.26%\n"
     ]
    }
   ],
   "source": [
    "evaluate(resnet18, train_loader, mode=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 66.51%\n"
     ]
    }
   ],
   "source": [
    "evaluate(resnet18, test_loader, mode=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet+LI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetLI(nn.Module):\n",
    "    def __init__(self, weights=\"DEFAULT\", freeze=False):\n",
    "        super(ResnetLI, self).__init__() \n",
    "        \n",
    "        resnet = models.resnet18(weights=weights)\n",
    "\n",
    "        if freeze:\n",
    "            for param in resnet.parameters():\n",
    "                param.require_grad = False\n",
    "\n",
    "        self.log = {\"W\": [], \"m\": [], \"v\": [], \"b\": []}\n",
    "        \n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.li1 = LateralInhibition(self.conv1.out_channels)\n",
    "        self.maxpool = resnet.maxpool\n",
    "\n",
    "        self.layer1 = self.convert_layer_blocks(resnet.layer1)\n",
    "        self.layer2 = self.convert_layer_blocks(resnet.layer2)\n",
    "        self.layer3 = self.convert_layer_blocks(resnet.layer3)\n",
    "        self.layer4 = self.convert_layer_blocks(resnet.layer4)\n",
    "        \n",
    "        # Copy all the non-convolutional parts of ResNet\n",
    "        self.avgpool = resnet.avgpool\n",
    "        self.fc = resnet.fc\n",
    "\n",
    "        # Add the \"plain\" li params to the log\n",
    "        self.log[\"W\"].append(self.li1.weights)\n",
    "        self.log[\"m\"].append(self.li1.m)\n",
    "        self.log[\"v\"].append(self.li1.v)\n",
    "        self.log[\"b\"].append(self.li1.b)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.li1(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def convert_layer_blocks(self, layer: nn.Sequential):\n",
    "        new_layer = []\n",
    "\n",
    "        for l in layer:\n",
    "            if isinstance(l, BasicBlock):\n",
    "                liblock = LIBlock(l)\n",
    "                new_layer.append(liblock)\n",
    "                \n",
    "                self.log[\"W\"].append(self.liblock.li.weights)\n",
    "                self.log[\"m\"].append(self.liblock.li.m)\n",
    "                self.log[\"v\"].append(self.liblock.li.v)\n",
    "                self.log[\"b\"].append(self.liblock.li.b)\n",
    "            else:\n",
    "                new_layer.append(l)\n",
    "\n",
    "        return nn.Sequential(*new_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnetLI = ResnetLI().to(device)\n",
    "train_model(resnetLI, 10, 2, 2, \"RESNET_LI\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = enumerate(train_loader)\n",
    "idx, (image, label) = next(batch)\n",
    "\n",
    "image, label = image.to(device), label.to(device)\n",
    "\n",
    "LI = LateralInhibition().cuda()\n",
    "output = LI(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 13\n",
    "img = deprocess(image[idx])\n",
    "display(img)\n",
    "\n",
    "li_img = deprocess(output[idx])\n",
    "display(li_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da2e2b763ce854165d4087dd5d7e36e1631e69a252c45c2f7bdfd54f03b8f37b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
